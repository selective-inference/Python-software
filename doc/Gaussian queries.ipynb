{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KKT conditions\n",
    "\n",
    "$$\n",
    "\\omega = \\nabla \\ell(o) + u + \\epsilon o.\n",
    "$$\n",
    "\n",
    "## Current terms used in selective MLE\n",
    "\n",
    "- `observed_score_state`: for LASSO this is $S=-X^TY$ (and for any linear regression), in general it should be\n",
    "$\\nabla \\ell(\\beta^*) - Q(\\beta^*)\\beta^*$, call this $A$\n",
    "\n",
    "- `opt_offset`: this is $\\hat{u}$ or (changed everywhere to `observed_subgrad`)\n",
    "\n",
    "- `opt_linear`: this is $\\nabla^2 \\ell(\\hat{\\beta}) + \\epsilon I$ restricted to \"selected\" subspace, call this $L$\n",
    "\n",
    "## Rewrite of KKT\n",
    "\n",
    "$$\n",
    "\\omega = Lo + S + u.\n",
    "$$\n",
    "\n",
    "## More terms in the code\n",
    "\n",
    "- Randomization precision `randomizer_prec` call this $\\Theta_{\\omega}=\\Sigma_{\\omega}^{-1}$ so $\\omega \\sim N(0, \\Theta^{-1})$.\n",
    "\n",
    "- `cond_cov`= $\\Sigma_{o|S,u}$, `cond_mean`, `cond_precision`=$\\Sigma_{o|S,u}^{-1}=\\Theta_{o|S,u}$:\n",
    "describe implied law of $o|S,u$. These are computed in `_setup_implied_gaussian`. Specifically, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Sigma_{o|S,u} = (L^T\\Theta L)^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- `regress_opt` (formerly `logdens_linear`) call this $A$: this is the regression of $o$ onto $S+u$, in the implied\n",
    "Gaussian given $u,S$ i.e.\n",
    "\n",
    "$$\n",
    "E[o|S,u] = A(S+u) = -\\Sigma_{o|S,u} L^T \\Theta_{\\omega}(S+u).\n",
    "$$\n",
    "\n",
    "- `cond_mean` is the conditional mean of $o|S,u$ evaluated at observed $S,u$: $A(S+u)_{obs}$. Or, `regress_opt_score(observed_score_state + observed_subgrad)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target related\n",
    "\n",
    "- `observed_target, target_cov, target_prec`: not much explanation needed $\\hat{\\theta}, \\Sigma_{\\hat{\\theta}}, \\Theta_{\\hat{\\theta}} = \\Sigma_{\\hat{\\theta}}^{-1}$\n",
    "\n",
    "- `target_score_cov`: $\\Sigma_{\\hat{\\theta},S}$\n",
    "\n",
    "- `regress_target`: regression of target onto score, formally this would be $\\Sigma_{\\hat{\\theta},S}\\Theta_S $ (transpose of usual way of writing regression, not in code yet), let's call it $B$ for now\n",
    "\n",
    "- `cov_product`: $\\Sigma_S \\Theta_{\\omega}$: product of score covariance and randomization precision.\n",
    "\n",
    "- `cov_score`: $\\Sigma_S$\n",
    "\n",
    "- `score_offset = observed_score_state + observed_subgrad`=$S+u$\n",
    "\n",
    "### In `selective_MLE`\n",
    "\n",
    "- `target_linear`: $\\Sigma_{S,\\hat{\\theta}}\\Theta_{\\hat{\\theta}}= \\Sigma_S B^T\\Theta_{\\hat{\\theta}}$ (changed name to `regress_score_target`)\n",
    "\n",
    "- `target_offset`: $S+u-\\Sigma_S B^T \\Theta_{\\hat{\\theta}} \\hat{\\theta} = S+u - \\Sigma_{S,\\hat{\\theta}} \\Theta_{\\hat{\\theta}} \\hat{\\theta}$ (changed name to `resid_score_target`)\n",
    "\n",
    "- `target_lin`: $A\\Sigma_S B^T \\Theta_{\\hat{\\theta}} = -(L^T\\Theta_{\\omega}L)^{-1} L^T\\Theta_{\\omega} \\Sigma_S B^T \\Theta_{\\hat{\\theta}}$ (changed name to `regress_opt_target`\n",
    "\n",
    "- `target_off`: $A(S+u - \\Sigma_S B^T \\Theta_{\\hat{\\theta}} \\hat{\\theta})$ `resid_opt_target`\n",
    "\n",
    "- `_P`: $\\Theta_{\\hat{\\theta}} B\\Sigma_S \\Theta_{\\omega} (S+u-\\Sigma_S B^T \\Theta_{\\hat{\\theta}} \\hat{\\theta}) = \\Theta_{\\hat{\\theta}} B\\Sigma_S \\Theta_{\\omega} (S+u) - \\Theta_{\\hat{\\theta}} B\\Sigma_S \\Theta_{\\omega} \\Sigma_S B^T \\Theta_{\\hat{\\theta}} \\hat{\\theta} = \\Theta_{\\hat{\\theta}} B\\Sigma_S \\Theta_{\\omega} (S+u) - \\Theta_{\\hat{\\theta}} B\\Sigma_S \\Theta_{\\omega} \\Sigma_{\\omega} \\Theta_{\\omega} \\Sigma_S B^T \\Theta_{\\hat{\\theta}} \\hat{\\theta} $.\n",
    "Let's call `_P` $\\xi$\n",
    "\n",
    "- `_prec`: $\\Theta_{\\hat{\\theta}} + \\Theta_{\\hat{\\theta}} B\\Sigma_S \\Theta_{\\omega} \\Sigma_S B^T \\Theta_{\\hat{\\theta}}\n",
    "- \\Theta_{\\hat{\\theta}} B \\Sigma_S A^T \\Theta_{o|S,u} A \\Sigma_S B^T \\Theta_{\\hat{\\theta}}$\n",
    "\n",
    "- `C`: something that can be computed with all of the above... I guess (but am not sure) that `_prec` is \n",
    "the precision of the (best case, no-selection) unbiased estimate of our target when we condition on $N,u$ \n",
    "\n",
    "- More precisely,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Theta_{\\hat{\\theta}} C &= \\xi + (A\\Sigma_S B^T \\Theta_{\\hat{\\theta}})^T L^T \\Theta_{\\omega} L (A\\Sigma_S B^T \\Theta_{\\hat{\\theta}})^T \\hat{\\theta} - (A\\Sigma_S B^T \\Theta_{\\hat{\\theta}})^T L^T \\Theta_{\\omega} L A(S+u) \\\\\n",
    "&= \\xi + \\Theta_{\\hat{\\theta}}B \\left(\\Sigma_S A^T L^T\\Theta_{\\omega} L A \\Sigma_S B^T \\Theta_{\\hat{\\theta}} \\hat{\\theta} - \\Sigma_S A^T L^T\\Theta_{\\omega} L A(S+u) \\right)  \\\\\n",
    "&= \\xi + \\Theta_{\\hat{\\theta}}B \\left(\\Sigma_S \\Theta_{\\omega} L (L^T\\Theta_{\\omega} L)^{-1} L^T \\Theta_{\\omega} \\Sigma_S B^T \\Theta_{\\hat{\\theta}} \\hat{\\theta} + \\Sigma_S \\Theta_{\\omega}L  A(S+u) \\right)  \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The expression $A(S+u)$ is `cond_mean` and the other term can be computed straightforwardly. We've used the fact\n",
    "$$\n",
    "A\\Sigma_S = -\\Sigma_{o|S,u}L^T\\Theta_{\\omega} \\Sigma_S =- (L^T\\Theta_{\\omega}L)^{-1}L^T\\Theta_{\\omega}\\Sigma_S\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "- Don't know what to sensibly call the last three things... but `_P` and `_prec` are the arguments to the\n",
    "optimization problem so these are what needs computing. I did change `_prec` to `prec_target_nosel`\n",
    "\n",
    "- `cov_target.dot(regress_opt_target.T.dot(prec_opt))`. This is\n",
    "\n",
    "$$-\\Sigma_{\\hat{\\theta}} \\Theta_{\\hat{\\theta}}B \\Sigma_S\\Theta_{\\omega} L (L^T\\Theta_{\\omega}L)^{-1} (L^T\\Theta_{\\omega} L) = B \\Sigma_S\\Theta_{\\omega} L$$\n",
    "\n",
    "- `regress_opt_target.T.dot(prec_opt)`. This is\n",
    "\n",
    "$$-\\Theta_{\\hat{\\theta}}B \\Sigma_S\\Theta_{\\omega} L (L^T\\Theta_{\\omega}L)^{-1} (L^T\\Theta_{\\omega} L) = \\Theta_{\\hat{\\theta}} B \\Sigma_S\\Theta_{\\omega} L$$\n",
    "\n",
    "- `regress_opt_target.T.dot(prec_opt).dot(regress_opt_target)`: This is\n",
    "\n",
    "$$\n",
    "\\Theta_{\\hat{\\theta}}B \\Sigma_S\\Theta_{\\omega} L (L^T\\Theta_{\\omega}L)^{-1} L^T\\Theta_{\\omega} \\Sigma_S B^T \\Theta_{\\hat{\\theta}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational considerations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: $\\Theta_{\\omega}^{1/2}$ is known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another potential downside to all this is that these matrices will generally be $p \\times p$. I think in `price_of_selection` I had written some way of doing part of this without having to form all of these matrices\n",
    "explicitly.  However, the difference of the last two matrices in `_prec` can be computed (if we know $\\Sigma_{\\omega}^{\\pm 1/2}$ as identity minus rank $E$ matrix I think and\n",
    "$$\n",
    "A^T\\Sigma_{o|S,u}A = \\Theta_{\\omega} L^T \\Sigma_{o|S,u} L \\Theta_{\\omega}\n",
    "$$\n",
    "so we want to compute\n",
    "$$\n",
    "\\Theta_{\\omega} - \\Theta_{\\omega} L^T \\Sigma_{o|S,u} L \\Theta_{\\omega} = \\Theta_{\\omega}^{1/2}(P - \\Theta_{\\omega}^{1/2}L^T (L^T\\Theta_{\\omega} L)^{-1} L\\Theta_{\\omega}^{1/2}) \\Theta_{\\omega}^{1/2}\n",
    "$$\n",
    "with $P$ projection onto $\\text{row}(\\Sigma_{\\omega})$. So we need to compute projection on to a $E$-dimensional\n",
    "subspace of $\\text{row}(\\Sigma_{\\omega})$. Morally, this makes sense even if $\\Sigma_{\\omega}$ is not full rank but seems a little sketchy.\n",
    "\n",
    "We might also try computing\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Sigma_S\\Theta_{\\omega}\\Sigma_S -  \\Sigma_S\\Theta_{\\omega} L^T \\Sigma_{o|S,u} L \\Theta_{\\omega} \\Sigma_S &= \\Sigma_S \\Theta_{\\omega}^{1/2}(P - \\Theta_{\\omega}^{1/2}L^T (L^T\\Theta_{\\omega} L)^{-1} L\\Theta_{\\omega}^{1/2}) \\Theta_{\\omega}^{1/2} \\Sigma_S \\\\\n",
    "&= \\Sigma_S \\Theta_{\\omega} \\Theta_{\\omega}^{-1/2}(P - \\Theta_{\\omega}^{1/2}L^T (L^T\\Theta_{\\omega} L)^{-1} L\\Theta_{\\omega}^{1/2}) \\Theta_{\\omega}^{-1/2} \\Theta_{\\omega} \\Sigma_S \\\\\n",
    "&= \\Sigma_S \\Theta_{\\omega} \\Sigma_{\\omega}^{1/2}(P - \\Theta_{\\omega}^{1/2}L^T (L^T\\Theta_{\\omega} L)^{-1} L\\Theta_{\\omega}^{1/2}) \\Sigma_{\\omega}^{1/2} \\Theta_{\\omega} \\Sigma_S \\\\\n",
    "&= \\Sigma_S \\Theta_{\\omega} (\\Sigma_{\\omega} - PL^T (L^T\\Theta_{\\omega} L)^{-1} LP)  \\Theta_{\\omega} \\Sigma_S \\\\\n",
    "&= \\Sigma_S \\Theta_{\\omega} (\\Sigma_{\\omega} - L^T (L^T\\Theta_{\\omega} L)^{-1} L)  \\Theta_{\\omega} \\Sigma_S \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Three matrices\n",
    "\n",
    "- All the computations above can be expressed of some target specific info like $B, \\Theta_{\\hat{\\theta}}, \\Sigma_{\\hat{\\theta}}, \\hat{\\theta}$ and\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "M_1 &= \\Sigma_S \\Theta_{\\omega} \\\\\n",
    "M_2 &= M_1 \\Sigma_{\\omega} M_1^T \\\\\n",
    "M_3 &= M_1 L (L^T\\Sigma_{\\omega}L)^{-1} L M_1^T\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
